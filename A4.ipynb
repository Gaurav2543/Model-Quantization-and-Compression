{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing quantization analysis...\n",
      "Loading model gpt2...\n",
      "Loading evaluation dataset...\n",
      "Loaded 2461 evaluation samples\n",
      "\n",
      "Evaluating original model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Computing perplexity: 100%|██████████| 2461/2461 [05:07<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to quantized_models/original...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully to quantized_models/original\n",
      "\n",
      "Evaluating selective quantization...\n",
      "Starting selective 8-bit quantization for first 5 layers...\n",
      "Quantized attention weights for layer 0\n",
      "Quantized FFN weights for layer 0\n",
      "Quantized attention weights for layer 1\n",
      "Quantized FFN weights for layer 1\n",
      "Quantized attention weights for layer 2\n",
      "Quantized FFN weights for layer 2\n",
      "Quantized attention weights for layer 3\n",
      "Quantized FFN weights for layer 3\n",
      "Quantized attention weights for layer 4\n",
      "Quantized FFN weights for layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Computing perplexity: 100%|██████████| 2461/2461 [05:04<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to quantized_models/selective_8bit...\n",
      "Model and tokenizer saved successfully to quantized_models/selective_8bit\n",
      "\n",
      "Evaluating 8-bit quantization...\n",
      "Starting 8-bit whole model quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Computing perplexity: 100%|██████████| 2461/2461 [05:04<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to quantized_models/quantized_8bit...\n",
      "Model and tokenizer saved successfully to quantized_models/quantized_8bit\n",
      "\n",
      "Generating analysis results...\n",
      "\n",
      "=== Quantization Evaluation Summary ===\n",
      "\n",
      "Size:\n",
      "original       :   474.70 MB (+0.0% change)\n",
      "selective_8bit :   415.64 MB (-12.4% change)\n",
      "quantized_8bit :   119.02 MB (-74.9% change)\n",
      "\n",
      "Latency:\n",
      "original       :    0.467 s (+0.0% change)\n",
      "selective_8bit :    0.459 s (-1.9% change)\n",
      "quantized_8bit :    0.454 s (-2.8% change)\n",
      "\n",
      "Memory Usage:\n",
      "original       :     0.17 MB (+0.0% change)\n",
      "selective_8bit :     0.00 MB (-100.0% change)\n",
      "quantized_8bit :     0.00 MB (-100.0% change)\n",
      "\n",
      "Perplexity:\n",
      "original       :    50.90 (+0.0% change)\n",
      "selective_8bit :    51.73 (+1.6% change)\n",
      "quantized_8bit :    58.66 (+15.2% change)\n",
      "\n",
      "Quantization analysis completed successfully!\n",
      "\n",
      "All models and results have been saved to: quantized_models\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "def load_model_and_tokenizer(model_name: str) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer with proper padding token configuration.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to load from HuggingFace\n",
    "    Returns:\n",
    "        tuple: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    print(f\"Loading model {model_name}...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "        padding_side='left',\n",
    "    )\n",
    "    \n",
    "    # Properly handle padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "        \n",
    "    return model, tokenizer\n",
    "\n",
    "def load_evaluation_data(max_samples: int = 3000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load and prepare evaluation dataset from WikiText-2.\n",
    "    \n",
    "    Args:\n",
    "        max_samples: Maximum number of samples to load\n",
    "    Returns:\n",
    "        list: List of text samples for evaluation\n",
    "    \"\"\"\n",
    "    print(\"Loading evaluation dataset...\")\n",
    "    try:\n",
    "        dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', trust_remote_code=True)\n",
    "        eval_data = [\n",
    "            text for text in dataset['validation']['text'] \n",
    "            if isinstance(text, str) and len(text.strip()) > 0\n",
    "        ][:max_samples]\n",
    "        \n",
    "        if not eval_data:\n",
    "            raise ValueError(\"No valid evaluation data found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        print(\"Falling back to synthetic data...\")\n",
    "        # Create synthetic data if dataset loading fails\n",
    "        eval_data = [\n",
    "            \"The quick brown fox jumps over the lazy dog.\",\n",
    "            \"A journey of a thousand miles begins with a single step.\",\n",
    "            \"To be or not to be, that is the question.\"\n",
    "        ] * (max_samples // 3 + 1)\n",
    "        eval_data = eval_data[:max_samples]\n",
    "    \n",
    "    print(f\"Loaded {len(eval_data)} evaluation samples\")\n",
    "    return eval_data\n",
    "\n",
    "def calculate_model_size(model: AutoModelForCausalLM, quantized_params: set = None, bits: int = None) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the model size in MB, accounting for both quantized and non-quantized parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to analyze\n",
    "        quantized_params: Set of parameter names that have been quantized\n",
    "        bits: Number of bits used for quantized parameters\n",
    "    Returns:\n",
    "        float: Model size in MB\n",
    "    \"\"\"\n",
    "    total_size = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if quantized_params and name in quantized_params:\n",
    "            # For quantized parameters, calculate size based on bit width\n",
    "            param_size = (param.nelement() * bits) / 8\n",
    "            param_size += 8  # Add overhead for scale and zero point\n",
    "        else:\n",
    "            # For non-quantized parameters, use actual size\n",
    "            param_size = param.nelement() * param.element_size()\n",
    "        total_size += param_size\n",
    "    return total_size / (1024 * 1024)  # Convert bytes to MB\n",
    "\n",
    "def quantize_tensor(tensor: torch.Tensor, bits: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Quantize a single tensor with improved stability and performance preservation.\n",
    "    \n",
    "    Args:\n",
    "        tensor: Input tensor to quantize\n",
    "        bits: Target bit width\n",
    "    Returns:\n",
    "        torch.Tensor: Quantized tensor in original dtype\n",
    "    \"\"\"\n",
    "    qmin = -(2 ** (bits - 1))\n",
    "    qmax = 2 ** (bits - 1) - 1\n",
    "    \n",
    "    tensor_flat = tensor.detach().flatten()\n",
    "    \n",
    "    # Calculate more conservative bounds\n",
    "    abs_max = abs(tensor_flat).max().item()\n",
    "    scale = abs_max / qmax\n",
    "    \n",
    "    if scale == 0:\n",
    "        return tensor.clone()\n",
    "    \n",
    "    # Quantize\n",
    "    tensor_q = torch.clamp(torch.round(tensor.detach() / scale), qmin, qmax)\n",
    "    \n",
    "    # Dequantize\n",
    "    tensor_d = tensor_q * scale\n",
    "    \n",
    "    return tensor_d.to(tensor.dtype)\n",
    "\n",
    "def quantize_whole_model(model: AutoModelForCausalLM, bits: int) -> Tuple[AutoModelForCausalLM, set]:\n",
    "    \"\"\"\n",
    "    Quantize model with improved stability.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to quantize\n",
    "        bits: Target bit width\n",
    "    Returns:\n",
    "        tuple: (quantized model, set of quantized parameter names)\n",
    "    \"\"\"\n",
    "    print(f\"Starting {bits}-bit whole model quantization...\")\n",
    "    quantized_params = set()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.dtype in [torch.float32, torch.float16]:\n",
    "                # Only quantize weights, not biases\n",
    "                if len(param.shape) >= 2:  # Skip bias terms\n",
    "                    if bits == 4:\n",
    "                        # For 4-bit, be more selective\n",
    "                        if param.numel() > 1000 and 'weight' in name:\n",
    "                            quantized_data = quantize_tensor(param.data, bits)\n",
    "                            param.data.copy_(quantized_data)\n",
    "                            quantized_params.add(name)\n",
    "                    else:\n",
    "                        quantized_data = quantize_tensor(param.data, bits)\n",
    "                        param.data.copy_(quantized_data)\n",
    "                        quantized_params.add(name)\n",
    "    \n",
    "    return model, quantized_params\n",
    "\n",
    "def quantize_selective_components(model: AutoModelForCausalLM, bits: int, num_layers: int = 5) -> Tuple[AutoModelForCausalLM, set]:\n",
    "    \"\"\"\n",
    "    Selectively quantize with improved stability.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to quantize\n",
    "        bits: Target bit width\n",
    "        num_layers: Number of layers to quantize\n",
    "    Returns:\n",
    "        tuple: (quantized model, set of quantized parameter names)\n",
    "    \"\"\"\n",
    "    print(f\"Starting selective {bits}-bit quantization for first {num_layers} layers...\")\n",
    "    quantized_params = set()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_layers):\n",
    "            try:\n",
    "                # Quantize only the weight matrices, not the biases\n",
    "                attn_name = f\"transformer.h.{i}.attn.c_attn.weight\"\n",
    "                if hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
    "                    attn_wts = model.transformer.h[i].attn.c_attn.weight\n",
    "                    quantized_attn_wts = quantize_tensor(attn_wts, bits)\n",
    "                    attn_wts.data.copy_(quantized_attn_wts)\n",
    "                    quantized_params.add(attn_name)\n",
    "                    print(f\"Quantized attention weights for layer {i}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to quantize attention weights for layer {i}: {str(e)}\")\n",
    "\n",
    "            try:\n",
    "                ffn_name = f\"transformer.h.{i}.mlp.c_fc.weight\"\n",
    "                if hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
    "                    ffn_wts = model.transformer.h[i].mlp.c_fc.weight\n",
    "                    quantized_ffn_wts = quantize_tensor(ffn_wts, bits)\n",
    "                    ffn_wts.data.copy_(quantized_ffn_wts)\n",
    "                    quantized_params.add(ffn_name)\n",
    "                    print(f\"Quantized FFN weights for layer {i}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to quantize FFN weights for layer {i}: {str(e)}\")\n",
    "    \n",
    "    return model, quantized_params\n",
    "\n",
    "def measure_latency(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, \n",
    "                   text: str, num_runs: int = 5) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Measure model inference latency and memory usage.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to evaluate\n",
    "        tokenizer: Tokenizer for input processing\n",
    "        text: Input text for inference\n",
    "        num_runs: Number of runs for averaging\n",
    "    Returns:\n",
    "        tuple: (average latency, average memory usage)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Warm-up run\n",
    "    with torch.no_grad():\n",
    "        model.generate(**inputs, max_new_tokens=20)\n",
    "    \n",
    "    latencies = []\n",
    "    memory_usage = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else gc.collect()\n",
    "        \n",
    "        memory_before = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "        \n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            model.generate(**inputs, max_new_tokens=20)\n",
    "        latencies.append(time.time() - start_time)\n",
    "        \n",
    "        memory_after = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "        memory_usage.append(memory_after - memory_before)\n",
    "    \n",
    "    return np.mean(latencies), np.mean(memory_usage)\n",
    "\n",
    "def compute_perplexity(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, \n",
    "                      eval_data: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Compute model perplexity on evaluation data.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to evaluate\n",
    "        tokenizer: Tokenizer for input processing\n",
    "        eval_data: List of evaluation texts\n",
    "    Returns:\n",
    "        float: Perplexity score\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(eval_data, desc=\"Computing perplexity\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            total_loss += outputs.loss.item() * inputs[\"input_ids\"].size(1)\n",
    "            total_tokens += inputs[\"input_ids\"].size(1)\n",
    "    \n",
    "    return float(torch.exp(torch.tensor(total_loss / total_tokens)))\n",
    "\n",
    "def evaluate_model_version(model: AutoModelForCausalLM, tokenizer: AutoTokenizer,\n",
    "                         eval_data: List[str], sample_text: str, \n",
    "                         quantized_params: set = None, bits: int = None) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a model version on all metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to evaluate\n",
    "        tokenizer: Tokenizer for input processing\n",
    "        eval_data: Evaluation dataset\n",
    "        sample_text: Text for latency testing\n",
    "        quantized_params: Set of quantized parameter names\n",
    "        bits: Bit width used for quantization\n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'size': calculate_model_size(model, quantized_params, bits),\n",
    "        'latency': measure_latency(model, tokenizer, sample_text)[0],\n",
    "        'memory_usage': measure_latency(model, tokenizer, sample_text)[1],\n",
    "        'perplexity': compute_perplexity(model, tokenizer, eval_data)\n",
    "    }\n",
    "\n",
    "def create_visualizations(results: Dict[str, Dict[str, float]]):\n",
    "    \"\"\"\n",
    "    Create and save visualization plots for all metrics.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of evaluation results\n",
    "    \"\"\"\n",
    "    metrics = ['size', 'latency', 'memory_usage', 'perplexity']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Model Quantization Performance Comparison', fontsize=16, y=1.02)\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    df = pd.DataFrame(results).T.reset_index()\n",
    "    df.columns = ['Model'] + metrics\n",
    "    \n",
    "    # Create subplots\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        # Create barplot\n",
    "        sns.barplot(\n",
    "            data=df,\n",
    "            x='Model',\n",
    "            y=metric,\n",
    "            hue='Model',\n",
    "            ax=ax,\n",
    "            legend=False\n",
    "        )\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(ax.patches):\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width()/2,\n",
    "                bar.get_height(),\n",
    "                f'{bar.get_height():.2f}',\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize=8\n",
    "            )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('quantization_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def print_evaluation_summary(results: Dict[str, Dict[str, float]]):\n",
    "    print(\"\\n=== Quantization Evaluation Summary ===\")\n",
    "    metrics = ['size', 'latency', 'memory_usage', 'perplexity']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
    "        baseline = results['original'][metric]\n",
    "        for model_type, values in results.items():\n",
    "            # Handle zero division cases\n",
    "            if baseline == 0:\n",
    "                change = 0 if values[metric] == 0 else float('inf')\n",
    "            else:\n",
    "                change = ((values[metric] - baseline) / baseline) * 100\n",
    "            \n",
    "            # Format output based on metric type\n",
    "            if metric == 'size':\n",
    "                print(f\"{model_type:15s}: {values[metric]:8.2f} MB ({change:+.1f}% change)\")\n",
    "            elif metric == 'latency':\n",
    "                print(f\"{model_type:15s}: {values[metric]:8.3f} s ({change:+.1f}% change)\")\n",
    "            elif metric == 'memory_usage':\n",
    "                print(f\"{model_type:15s}: {values[metric]:8.2f} MB ({change:+.1f}% change)\")\n",
    "            else:  # perplexity\n",
    "                print(f\"{model_type:15s}: {values[metric]:8.2f} ({change:+.1f}% change)\")\n",
    "\n",
    "def save_model(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, path: str):\n",
    "    \"\"\"\n",
    "    Save model and tokenizer to disk.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to save\n",
    "        tokenizer: Tokenizer to save\n",
    "        path: Directory path to save to\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Saving model to {path}...\")\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        print(f\"Model and tokenizer saved successfully to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {str(e)}\")\n",
    "\n",
    "\n",
    "output_dir = \"quantized_models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Initialize\n",
    "print(\"\\nInitializing quantization analysis...\")\n",
    "output_dir = \"quantized_models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\"gpt2\")\n",
    "eval_data = load_evaluation_data(max_samples=3000)\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "results = {}\n",
    "original_state = {name: param.clone() for name, param in model.named_parameters()}\n",
    "\n",
    "# Evaluate and save original model\n",
    "print(\"\\nEvaluating original model...\")\n",
    "results['original'] = evaluate_model_version(model, tokenizer, eval_data, sample_text)\n",
    "save_model(model, tokenizer, os.path.join(output_dir, \"original\"))\n",
    "\n",
    "# Evaluate and save selective quantization\n",
    "print(\"\\nEvaluating selective quantization...\")\n",
    "model_selective, params_selective = quantize_selective_components(model, bits=8)\n",
    "results['selective_8bit'] = evaluate_model_version(\n",
    "    model_selective, tokenizer, eval_data, sample_text, params_selective, 8\n",
    ")\n",
    "save_model(model_selective, tokenizer, os.path.join(output_dir, \"selective_8bit\"))\n",
    "\n",
    "# Restore original state\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        param.data.copy_(original_state[name])\n",
    "\n",
    "# Evaluate and save 8-bit whole model quantization\n",
    "print(\"\\nEvaluating 8-bit quantization...\")\n",
    "model_8bit, params_8bit = quantize_whole_model(model, bits=8)\n",
    "results['quantized_8bit'] = evaluate_model_version(\n",
    "    model_8bit, tokenizer, eval_data, sample_text, params_8bit, 8\n",
    ")\n",
    "save_model(model_8bit, tokenizer, os.path.join(output_dir, \"quantized_8bit\"))\n",
    "\n",
    "# Create visualizations and save results\n",
    "print(\"\\nGenerating analysis results...\")\n",
    "create_visualizations(results)\n",
    "print_evaluation_summary(results)\n",
    "\n",
    "# Save the quantization results\n",
    "results_df = pd.DataFrame(results).round(4)\n",
    "results_df.to_csv(os.path.join(output_dir, \"quantization_results.csv\"))\n",
    "\n",
    "print(\"\\nQuantization analysis completed successfully!\")\n",
    "print(f\"\\nAll models and results have been saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating original model...\n",
      "Loading model gpt2...\n",
      "Loading evaluation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2461 evaluation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Computing perplexity: 100%|██████████| 2461/2461 [00:21<00:00, 114.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to bitsandbytes_models/original...\n",
      "Model and tokenizer saved successfully to bitsandbytes_models/original\n",
      "\n",
      "Evaluating 8-bit quantization...\n",
      "Loading model gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "2024-11-17 13:12:30.801036: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-17 13:12:30.801093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-17 13:12:30.802558: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-17 13:12:30.811747: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-17 13:12:32.902300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Computing perplexity: 100%|██████████| 2461/2461 [01:58<00:00, 20.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to bitsandbytes_models/8bit...\n",
      "Model and tokenizer saved successfully to bitsandbytes_models/8bit\n",
      "\n",
      "Evaluating NF4 quantization...\n",
      "Loading model gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Computing perplexity: 100%|██████████| 2461/2461 [00:41<00:00, 59.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to bitsandbytes_models/nf4...\n",
      "Model and tokenizer saved successfully to bitsandbytes_models/nf4\n",
      "\n",
      "=== Bitsandbytes Quantization Evaluation Summary ===\n",
      "\n",
      "Size:\n",
      "original       :   474.70 (+0.0% change)\n",
      "bitsandbytes_8bit:   156.35 (-67.1% change)\n",
      "bitsandbytes_nf4:   115.85 (-75.6% change)\n",
      "\n",
      "Latency:\n",
      "original       :     0.16 (+0.0% change)\n",
      "bitsandbytes_8bit:     0.71 (+357.2% change)\n",
      "bitsandbytes_nf4:     0.27 (+69.9% change)\n",
      "\n",
      "Memory Usage:\n",
      "original       :    -0.17 (-0.0% change)\n",
      "bitsandbytes_8bit:     0.00 (-100.0% change)\n",
      "bitsandbytes_nf4:     0.00 (-100.0% change)\n",
      "\n",
      "Perplexity:\n",
      "original       :    50.90 (+0.0% change)\n",
      "bitsandbytes_8bit:    51.15 (+0.5% change)\n",
      "bitsandbytes_nf4:    54.45 (+7.0% change)\n",
      "\n",
      "All models and results have been saved to: bitsandbytes_models\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import bitsandbytes as bnb\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "def load_model_and_tokenizer(model_name: str, quantization_config: BitsAndBytesConfig = None) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    Load model with optional quantization configuration.\n",
    "    \"\"\"\n",
    "    print(f\"Loading model {model_name}...\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    if quantization_config:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map={\"\": device}  # Modified device mapping\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        padding_side='left',\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def compute_perplexity(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, \n",
    "                      eval_data: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Compute model perplexity.\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    device = next(model.parameters()).device  # Get model's device\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(eval_data, desc=\"Computing perplexity\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to correct device\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            total_loss += outputs.loss.item() * inputs[\"input_ids\"].size(1)\n",
    "            total_tokens += inputs[\"input_ids\"].size(1)\n",
    "    \n",
    "    return float(torch.exp(torch.tensor(total_loss / total_tokens)))\n",
    "\n",
    "def measure_latency(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, \n",
    "                   text: str, num_runs: int = 5) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Measure inference latency and memory usage.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device  # Get model's device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to correct device\n",
    "    \n",
    "    # Warm-up run\n",
    "    with torch.no_grad():\n",
    "        model.generate(**inputs, max_new_tokens=20)\n",
    "    \n",
    "    latencies = []\n",
    "    memory_usage = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            gc.collect()\n",
    "        \n",
    "        memory_before = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "        \n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            model.generate(**inputs, max_new_tokens=20)\n",
    "        latencies.append(time.time() - start_time)\n",
    "        \n",
    "        memory_after = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "        memory_usage.append(memory_after - memory_before)\n",
    "    \n",
    "    return np.mean(latencies), np.mean(memory_usage)\n",
    "\n",
    "\n",
    "def load_evaluation_data(max_samples: int = 3000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load evaluation dataset.\n",
    "    \"\"\"\n",
    "    print(\"Loading evaluation dataset...\")\n",
    "    try:\n",
    "        dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', trust_remote_code=True)\n",
    "        eval_data = [\n",
    "            text for text in dataset['validation']['text'] \n",
    "            if isinstance(text, str) and len(text.strip()) > 0\n",
    "        ][:max_samples]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        eval_data = [\n",
    "            \"The quick brown fox jumps over the lazy dog.\",\n",
    "            \"A journey of a thousand miles begins with a single step.\",\n",
    "            \"To be or not to be, that is the question.\"\n",
    "        ] * (max_samples // 3 + 1)\n",
    "        eval_data = eval_data[:max_samples]\n",
    "    \n",
    "    print(f\"Loaded {len(eval_data)} evaluation samples\")\n",
    "    return eval_data\n",
    "\n",
    "def get_model_size(model: AutoModelForCausalLM) -> float:\n",
    "    \"\"\"\n",
    "    Calculate model size in MB.\n",
    "    \"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    return param_size / (1024 * 1024)\n",
    "\n",
    "def evaluate_model_version(model: AutoModelForCausalLM, tokenizer: AutoTokenizer,\n",
    "                         eval_data: List[str], sample_text: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model on all metrics.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'size': get_model_size(model),\n",
    "        'latency': measure_latency(model, tokenizer, sample_text)[0],\n",
    "        'memory_usage': measure_latency(model, tokenizer, sample_text)[1],\n",
    "        'perplexity': compute_perplexity(model, tokenizer, eval_data)\n",
    "    }\n",
    "\n",
    "def create_visualizations(results: Dict[str, Dict[str, float]]):\n",
    "    \"\"\"\n",
    "    Create comparison visualizations.\n",
    "    \"\"\"\n",
    "    metrics = ['size', 'latency', 'memory_usage', 'perplexity']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Quantization Performance Comparison (Bitsandbytes)', fontsize=16, y=1.02)\n",
    "    \n",
    "    df = pd.DataFrame(results).T.reset_index()\n",
    "    df.columns = ['Model'] + metrics\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        sns.barplot(\n",
    "            data=df,\n",
    "            x='Model',\n",
    "            y=metric,\n",
    "            hue='Model',\n",
    "            ax=ax,\n",
    "            legend=False\n",
    "        )\n",
    "        \n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        for i, bar in enumerate(ax.patches):\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width()/2,\n",
    "                bar.get_height(),\n",
    "                f'{bar.get_height():.2f}',\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize=8\n",
    "            )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bitsandbytes_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def print_evaluation_summary(results: Dict[str, Dict[str, float]]):\n",
    "    \"\"\"\n",
    "    Print evaluation summary.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Bitsandbytes Quantization Evaluation Summary ===\")\n",
    "    metrics = ['size', 'latency', 'memory_usage', 'perplexity']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
    "        baseline = results['original'][metric]\n",
    "        for model_type, values in results.items():\n",
    "            if baseline == 0:\n",
    "                change = 0 if values[metric] == 0 else float('inf')\n",
    "            else:\n",
    "                change = ((values[metric] - baseline) / baseline) * 100\n",
    "            print(f\"{model_type:15s}: {values[metric]:8.2f} ({change:+.1f}% change)\")\n",
    "\n",
    "def save_model(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, path: str):\n",
    "    \"\"\"\n",
    "    Save model and tokenizer to disk with quantization config.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to save\n",
    "        tokenizer: Tokenizer to save\n",
    "        path: Directory path to save to\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Saving model to {path}...\")\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        \n",
    "        # Save quantization configuration if it exists\n",
    "        if hasattr(model, 'config') and hasattr(model.config, 'quantization_config'):\n",
    "            model.config.save_pretrained(path)\n",
    "            \n",
    "        print(f\"Model and tokenizer saved successfully to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {str(e)}\")\n",
    "\n",
    "# Initialize\n",
    "model_name = \"gpt2\"\n",
    "output_dir = \"bitsandbytes_models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "results = {}\n",
    "\n",
    "# Original model\n",
    "print(\"\\nEvaluating original model...\")\n",
    "model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "eval_data = load_evaluation_data()\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "results['original'] = evaluate_model_version(model, tokenizer, eval_data, sample_text)\n",
    "save_model(model, tokenizer, os.path.join(output_dir, \"original\"))\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "# 8-bit quantization\n",
    "print(\"\\nEvaluating 8-bit quantization...\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "model_8bit, tokenizer = load_model_and_tokenizer(model_name, quantization_config)\n",
    "results['bitsandbytes_8bit'] = evaluate_model_version(model_8bit, tokenizer, eval_data, sample_text)\n",
    "save_model(model_8bit, tokenizer, os.path.join(output_dir, \"8bit\"))\n",
    "del model_8bit\n",
    "gc.collect()\n",
    "\n",
    "# NF4 quantization\n",
    "print(\"\\nEvaluating NF4 quantization...\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "model_nf4, tokenizer = load_model_and_tokenizer(model_name, quantization_config)\n",
    "results['bitsandbytes_nf4'] = evaluate_model_version(model_nf4, tokenizer, eval_data, sample_text)\n",
    "save_model(model_nf4, tokenizer, os.path.join(output_dir, \"nf4\"))\n",
    "\n",
    "# Create visualizations and print summary\n",
    "create_visualizations(results)\n",
    "print_evaluation_summary(results)\n",
    "\n",
    "# Save the quantization results\n",
    "results_df = pd.DataFrame(results).round(4)\n",
    "results_df.to_csv(os.path.join(output_dir, \"bitsandbytes_results.csv\"))\n",
    "\n",
    "print(f\"\\nAll models and results have been saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
